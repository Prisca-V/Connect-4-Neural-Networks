components:
- components:
  - layout_properties: {grid_position: 'ODUTHU,LBRXHP'}
    name: spacer_7
    properties: {height: 68}
    type: Spacer
  - components:
    - layout_properties: {grid_position: 'SMNRIB,ZFSSST'}
      name: headline_1
      properties: {role: game-title, text: Report}
      type: Label
    - event_bindings: {}
      layout_properties: {grid_position: 'SMNRIB,HITQSP'}
      name: Play_game
      properties: {align: right, icon_align: right, role: elevated-button, text: Play Game}
      type: Button
    - layout_properties: {grid_position: 'NUIDAF,LCBBTA'}
      name: spacer_1
      properties: {height: 32}
      type: Spacer
    layout_properties: {grid_position: 'RHYSQN,CUJXKE'}
    name: column_panel_3
    properties: {}
    type: ColumnPanel
  - layout_properties: {grid_position: 'UUJPKQ,FKAUYJ'}
    name: lbl_dataGen
    properties: {role: report-title, text: Data Generation}
    type: Label
  - components:
    - components: []
      layout_properties: {grid_position: 'OGNZIQ,BQHTUN'}
      name: data_den_text
      properties:
        background: theme:Light Overlay 1
        content: |
          The dataset is generated by having two agents play Connect Four against each other using Monte Carlo Tree Search (MCTS). To increase diversity, each game may begin with a few random opening moves. These random moves are not included in the dataset , only positions that result from MCTS decisions are stored.
        role: report-body
        spacing:
          padding: [null, null, null, 13]
      type: RichText
    - layout_properties: {grid_position: 'LPUCRU,LCOUGH'}
      name: rich_text_4
      properties:
        background: theme:Light Overlay 1
        content: After the opening phase, MCTS selects moves based on a configurable search depth that controls how strong the agent plays. At each valid turn, the current board is encoded from the perspective of the player to move using a 6×7×2 tensor. The first channel represents the current player’s discs, and the second channel represents the opponent’s discs. This canonical encoding removes the need for a separate turn indicator and allows a single model to learn both players’ strategies.
        role: report-body
        spacing:
          padding: [null, null, null, 13]
      type: RichText
    - data_bindings: []
      event_bindings: {}
      layout_properties: {grid_position: 'XQWEVP,VSTNRA'}
      name: rich_text_2
      properties:
        background: theme:Light Overlay 1
        content: |-
          To avoid storing duplicate positions, each board state is converted into a byte hash and used as a key in a lookup table. Instead of saving repeated boards multiple times, the system aggregates them. For each unique state, a 7-element count vector records how often MCTS selects each column. The final label can be assigned using a majority vote, and positions with low agreement can optionally be filtered out.
          The generator also supports horizontal symmetry augmentation by mirroring board states and corresponding column labels. Checkpoints are saved periodically so generation can resume if interrupted. After all games are completed, the aggregated states are converted into feature tensors and label arrays, resulting in a deduplicated and policy-consistent dataset suitable for supervised training.
        format: restricted_html
        role: report-body
        spacing:
          padding: [null, null, null, 13]
        tooltip: ''
      type: RichText
    layout_properties: {grid_position: 'IJPOSL,EMJLFU'}
    name: outlined_card_5
    properties: {background: '', role: report-card}
    type: ColumnPanel
  - components: []
    layout_properties: {grid_position: 'IBKSZC,NXMORG'}
    name: Data_Generation
    properties: {}
    type: LinearPanel
  - components:
    - name: lbl_mdl_bld
      properties: {role: report-title, text: Model Building}
      type: Label
    - components:
      - layout_properties: {grid_position: 'KPMLVQ,MVHTOR'}
        name: lbl_cnn
        properties: {bold: false, role: report-title, text: Convolutional Neural Network(CNN), underline: false}
        type: Label
      - layout_properties: {grid_position: 'IOMQTA,BSCAXG'}
        name: label_12
        properties: {role: report-subhead, text: Model Overview}
        type: Label
      - layout_properties: {grid_position: 'JUUJDD,PTYSIG'}
        name: cnn_text
        properties:
          content: |
            <p>
            For this project, I built a Convolutional Neural Network (CNN) to learn Connect 4 strategies from MCTS data.
            </p>

            <p>
            CNNs learn through localized convolutional filters that detect spatial patterns. This makes them a natural fit for board games like Connect 4, where winning configurations consist of spatially local patterns such as horizontal, vertical, and diagonal four-in-a-row sequences.
            </p>

            <p>
            In my model, the early layers detect simple piece configurations, the middle layers detect partial threats (three-in-a-row, open-ended patterns), and the deeper layers combine these into higher-level tactical patterns.
            </p>
          format: restricted_html
          role: report-body
          spacing:
            padding: [null, 0, null, null]
        type: RichText
      - layout_properties: {grid_position: 'XCZUYX,GWRCUN'}
        name: label_13
        properties: {role: report-subhead, text: Model Architecture}
        type: Label
      - layout_properties: {grid_position: 'XFFNIZ,SXMXLQ'}
        name: report_body_1
        properties:
          content: |-
            <p><b>CNN Configuration</b></p>

            <p class="anvil-role-report-body">
            Below is the final CNN architecture used to predict the best Connect 4 move (7-class classification, one logit per column).
            </p>

            <p><b>Final Architecture (PyTorch-style)</b></p>
            <pre style="background: rgba(15,23,42,0.04); border: 1px solid rgba(15,23,42,0.08); border-radius: 14px; padding: 12px 14px; overflow-x:auto; font-size: 13.5px; line-height: 1.5;">
            Input: 6 × 7 board (2 channels)

            Conv Block 1:
              Conv2d(2 → 64, kernel=3×3, padding=1)
              ReLU

            Conv Block 2:
              Conv2d(64 → 128, kernel=3×3, padding=1)
              ReLU

            Conv Block 3:
              Conv2d(128 → 256, kernel=3×3, padding=1)
              ReLU

            Flatten:
              256 × 6 × 7 = 10,752

            Fully Connected Layers:
              Linear(10,752 → 256)
              Dropout(0.4)
              Linear(256 → 128)
              Dropout(0.3)
              Linear(128 → 7)

            Output:
              Clamp logits to [-10, 10]
              Softmax → probability distribution over columns
            </pre>

            <div style="height:10px;"></div>

            <p><b>Key Design Decisions</b></p>
            <ul>
              <li><b>3×3 kernels:</b> capture local spatial relationships (adjacent pieces and short threats).</li>
              <li><b>Padding = 1:</b> preserves board dimensions (6×7) across convolution layers.</li>
              <li><b>Increasing channel depth (64 → 128):</b> enables learning richer pattern combinations (horizontal/vertical/diagonal motifs).</li>
              <li><b>Batch Normalization:</b> stabilizes training and improves convergence.</li>
              <li><b>Dropout (0.3):</b> reduces overfitting by regularizing the dense layers.</li>
              <li><b>Final output (7 units):</b> produces one logit per column (softmax for move selection).</li>
            </ul>
          format: restricted_html
          role: report-body
        type: RichText
      - layout_properties: {grid_position: 'XJRAKV,XFTBTF'}
        name: report_subhead_1
        properties: {role: report-subhead, text: Training Process}
        type: Label
      - layout_properties: {grid_position: 'LWYIDX,OGZVPT'}
        name: report_body_4
        properties:
          content: |
            <p><b>Dataset</b></p>

            <ul>
              <li>Total board positions: <b>131,980</b></li>
              <li>Board shape: 6 × 7</li>
              <li>Labels: best move (column 0–6) determined by MCTS</li>
            </ul>

            <p>
            The dataset was shuffled and split into training and validation sets. Board states were converted into the 6 × 7 × 2 format and fed directly into the CNN.
            </p>

            <div style="height:10px;"></div>

            <p><b>Training Details</b></p>

            <ul>
              <li>Framework: PyTorch</li>
              <li>Optimizer: Adam with learning rate 0.0001</li>
              <li>Early stopping enabled</li>
              <li>Model saved as: <span style="font-family: monospace; color:#1F8F95;">connect4_cnn_pytorch.pt</span></li>
            </ul>

            <div style="height:10px;"></div>

            <p><b>Training Progression</b></p>

            <ul>
              <li>Initial validation loss: 1.7639</li>
              <li>Final validation loss: 1.6029</li>
              <li>Early stopping triggered at epoch 46</li>
            </ul>

            <p>
            Both training and validation loss decreased steadily, indicating stable convergence. The relatively small gap between training and validation loss suggests limited overfitting and good generalization.
            </p>
          format: restricted_html
          role: report-body
        type: RichText
      - layout_properties: {grid_position: 'LUXLXP,EDRRNZ'}
        name: report_subhead_2
        properties: {role: caption, text: What Worked Well}
        type: Label
      - layout_properties: {grid_position: 'TFFWEU,GTYTAS'}
        name: report_body_5
        properties:
          content: |-
            <p><b>Local Pattern Recognition</b></p>

            <p>
            Convolutional neural networks are highly effective at identifying spatial patterns. The model quickly learned to recognize horizontal threats, vertical stacking opportunities, diagonal formations, and immediate blocking moves. Because Connect 4 is fundamentally spatial, convolutional filters are well suited to this task.
            </p>

            <div style="height:10px;"></div>

            <p><b>Stable Convergence</b></p>

            <p>
            Training progressed smoothly and consistently. Validation loss decreased steadily across epochs with no major oscillations. Batch normalization significantly improved stability and learning speed.
            </p>

            <div style="height:10px;"></div>

            <p><b>Efficient Architecture</b></p>

            <p>
            The model achieved strong performance using only three convolutional layers and two fully connected layers.
            </p>

            <p>
            Compared to the transformer model, the CNN is simpler, faster to train, and requires less computational resources.
            </p>
          format: restricted_html
          role: report-body
        type: RichText
      - layout_properties: {grid_position: 'EIBRIU,POMYEP'}
        name: label_14
        properties: {role: caption, text: What Worked Poorly}
        type: Label
      - layout_properties: {grid_position: 'SPMYTF,DRPJIR'}
        name: report_body_6
        properties:
          content: |-
            <p><b>Limited Long Range Strategic Reasoning</b></p>

            <p>
            Convolutional neural networks build global understanding by stacking local filters. While deeper layers expand the receptive field, the model still struggles with planning moves that require long term strategy.
            </p>

            <p>
            Like the transformer model, it performs well at identifying immediate wins and blocks, but has difficulty with multi step traps, sacrificial setups, and long term positional advantages.
            </p>

            <div style="height:10px;"></div>

            <p><b>Ambiguous Midgame Boards</b></p>

            <p>
            Analysis of the difficult board examples shows many playable columns with no immediate win and multiple strategically reasonable moves. In these positions, the model often predicts a move different from MCTS. For example, low confidence predictions (approximately 15 percent) can reflect confusion between symmetric options or the selection of playable but non optimal columns. These moves should not be considered incorrect. Instead, they represent reasonable alternatives that differ from the highest visit MCTS choice.
            </p>
          format: restricted_html
          role: report-body
        type: RichText
      - layout_properties: {grid_position: 'JYOIFT,RUJUGR'}
        name: label_15
        properties: {role: report-subhead, text: Board Analysis}
        type: Label
      - layout_properties: {grid_position: 'WERNGA,BPHLTF'}
        name: report_body_7
        properties:
          content: |-
            <p><b>Difficult Boards</b></p>

            <p>
            The most challenging board positions tend to share the following characteristics:
            </p>

            <ul>
              <li>Many valid moves</li>
              <li>No forced win</li>
              <li>Strategic depth requiring two to three moves ahead</li>
              <li>Symmetry or near symmetry</li>
            </ul>
          format: restricted_html
          role: report-body
        type: RichText
      - layout_properties: {grid_position: 'MNWUCX,FLMMEX'}
        name: image_7
        properties: {display_mode: fill_width, source: _/theme/Screenshot 2026-02-15 200851.png}
        type: Image
      - layout_properties: {grid_position: 'MNWUCX,HMBOOQ'}
        name: image_8
        properties: {display_mode: fill_width, source: _/theme/Screenshot 2026-02-15 201056.png}
        type: Image
      - layout_properties: {grid_position: 'HYHHKF,EELXLE'}
        name: report_body_8
        properties:
          content: |-
            <p>
            The CNN sometimes distributes probability across multiple reasonable columns, resulting in low confidence predictions.
            </p>

            <p>
            This reflects a limitation of supervised learning on static snapshots: the CNN observes only the current board state and does not evaluate future consequences.
            </p>
          format: restricted_html
          role: report-body
        type: RichText
      - layout_properties: {grid_position: 'YNAIID,YYXDRU'}
        name: label_16
        properties: {role: report-subhead, text: Final Performance}
        type: Label
      - layout_properties: {grid_position: 'ZOBXSL,PMKLII'}
        name: rich_text_1
        properties:
          content: |-
            <p><b>Training Results</b></p>

            <ul>
              <li>Final training loss: <b>1.5069</b></li>
              <li>Final validation loss: <b>1.6029</b></li>
            </ul>

            <p>
            The CNN significantly reduces loss relative to an untrained model and demonstrates strong learning of tactical Connect 4 patterns. The close alignment between training and validation loss suggests:
            </p>

            <ul>
              <li>Good generalization</li>
              <li>Effective regularization (dropout 0.3)</li>
              <li>Appropriate model capacity</li>
            </ul>
          format: restricted_html
          role: report-body
        type: RichText
      name: column_panel_1
      properties: {role: report-card}
      type: ColumnPanel
    - name: spacer_3
      properties: {height: 32}
      type: Spacer
    - layout_properties: {}
      name: spacer_6
      properties: {height: 24.5}
      type: Spacer
    - components:
      - layout_properties: {grid_position: 'IORYYM,ADVEVP'}
        name: lbl_trans
        properties: {bold: false, role: report-title, text: Transformer Model, underline: false}
        type: Label
      - layout_properties: {grid_position: 'KBTTIG,AHHGUR'}
        name: label_3
        properties: {role: report-subhead, text: Model Overview}
        type: Label
      - components: []
        layout_properties: {grid_position: 'ARVPXR,TVKYUI'}
        name: trans_text
        properties:
          content: |
            For this project, I built a Vision Transformer (ViT) to learn Connect 4 strategy from Monte Carlo Tree Search (MCTS) data. The transformer treats the Connect 4 board as an image and uses self-attention mechanisms to identify winning patterns.Unlike CNNs that use fixed convolutional filters, transformers use attention mechanisms that can dynamically focus on any part of the board. This is particularly useful for Connect 4, where the importance of a position depends heavily on context. A piece in column 3 might be critical for blocking an opponent's diagonal, but irrelevant in another game state.
          format: restricted_html
          role: report-body
        type: RichText
      - layout_properties: {grid_position: 'BPRFHT,NDFGFZ'}
        name: label_5
        properties: {role: report-subhead, text: Model Architecture}
        type: Label
      - layout_properties: {grid_position: 'LNYCYQ,LYCYXU'}
        name: label_8
        properties: {role: caption, text: Input Representation}
        type: Label
      - layout_properties: {grid_position: 'MCOPVF,QFNHNC'}
        name: rich_text_6
        properties:
          content: |-
            <ul>
              <li><b>Board encoding:</b> 6×7×2</li>
              <li><b>Channel 0:</b> Player +1's pieces</li>
              <li><b>Channel 1:</b> Player -1's pieces</li>
              <li><b>Empty cells:</b> 0 in both channels</li>
              <li><b>Why two channels?</b> Helps the model distinguish players more clearly than a single-channel encoding.</li>
            </ul>
          format: restricted_html
          role: report-body
        type: RichText
      - layout_properties: {grid_position: 'HDGIPR,WCGZAH'}
        name: label_6
        properties: {role: caption, text: Patch Strategy}
        type: Label
      - layout_properties: {grid_position: 'OYCOPJ,ZYPULH'}
        name: rich_text_5
        properties:
          content: |-
            <ul>
              <li><b>Patch size:</b> 3×3 with stride 2 (overlapping patches)</li>
              <li><b>Total patches:</b> 9 (3 rows × 3 columns)</li>
              <li><b>Values per patch:</b> 3×3×2 = 18</li>
              <li><b>Why overlapping?</b> Overlapping patches capture 4-in-a-row patterns that span multiple regions of the board.</li>
            </ul>
          format: restricted_html
          role: report-body
        type: RichText
      - layout_properties: {grid_position: 'VYOLKM,TAGHEV'}
        name: label_7
        properties: {role: caption, text: Transformer Configuration}
        type: Label
      - layout_properties: {grid_position: 'AFFZWQ,JHZAGJ'}
        name: rich_text_7
        properties:
          content: |-
            <p><b>After experimentation, I settled on these hyperparameters:</b></p>

            <ul>
              <li><b>Hidden Dimension:</b> 64</li>
              <li><b>Transformer Layers:</b> 4</li>
              <li><b>Attention Heads:</b> 8 — allows the model to attend to horizontal, vertical, and diagonal patterns simultaneously</li>
              <li><b>MLP Dimension:</b> 128 (2× hidden dimension for feedforward network)</li>
              <li><b>Dropout:</b> 0.2 — regularization to prevent overfitting</li>
            </ul>
          format: restricted_html
          role: report-body
        type: RichText
      - layout_properties: {grid_position: 'OVUONC,OHEHIB'}
        name: label_4
        properties: {role: report-subhead, text: Training Process}
        type: Label
      - layout_properties: {grid_position: 'CMMHGY,ZQMJFB'}
        name: rich_text_8
        properties:
          content: |-
            <p><b>Dataset:</b></p>
            <ul>
              <li>MCTS-generated positions</li>
              <li>Total board positions: 131,980</li>
              <li>Training samples: 105,584 (80%)</li>
              <li>Validation samples: 26,396 (20%)</li>
              <li>Labels: Best move (column 0–6) determined by MCTS visit counts</li>
            </ul>

            <p>
            The dataset was loaded and preprocessed by decoding each board position from its binary representation into the 6×7×2 format. Each board was then converted into patches (9 patches of size 18, representing flattened 3×3×2 patches). The data was randomly shuffled and split into training and validation sets with an 80/20 split. Training proceeded in batches of 128, resulting in 825 training batches and 207 validation batches per epoch.
            </p>

            <p><b>Training Details:</b></p>
            <ul>
              <li>Framework: PyTorch (switched from TensorFlow for AWS deployment compatibility)</li>
              <li>Batch size: 128</li>
              <li>Optimizer: Adam with learning rate 0.0001</li>
              <li>Early stopping: Patience of 5 epochs monitoring validation loss</li>
            </ul>
          format: restricted_html
          role: report-body
        type: RichText
      - layout_properties: {grid_position: 'DYGEBF,DZIDCN'}
        name: label_9
        properties: {role: caption, text: What Worked Well}
        type: Label
      - layout_properties: {grid_position: 'OCKJFN,ZFYYNK'}
        name: rich_text_9
        properties:
          content: |-
            <p><b>Overlapping Patches</b></p>
            <p>
            The 3×3 patches with stride 2 proved effective. Because Connect 4 requires detecting 4-in-a-row patterns, overlapping patches helped the model see connections between adjacent board regions.
            </p>

            <p><b>Multiple Attention Heads</b></p>
            <p>
            Using 8 attention heads allowed the model to simultaneously look for horizontal threats (rows), vertical builds (columns), diagonal patterns, and blocking opportunities.
            </p>

            <p><b>Balanced Model Size</b></p>
            <p>
            Hidden dimension of 64 with 4 layers struck a good balance. It was large enough to capture complex patterns, but small enough to train quickly and avoid overfitting.
            </p>
          format: restricted_html
          role: report-body
        type: RichText
      - layout_properties: {grid_position: 'WRLHRU,HDIWFO'}
        name: label_10
        properties: {role: caption, text: What Worked Poorly}
        type: Label
      - layout_properties: {grid_position: 'GRKCJP,BAOFPS'}
        name: rich_text_10
        properties:
          content: |-
            <p><b>Initial TensorFlow Deployment Issues</b></p>
            <p>
            Spent significant time troubleshooting model serialization. TensorFlow 2.9.1 (required by Docker) couldn't load models saved in TF 2.18+. The .h5, .keras, and SavedModel formats all had version compatibility issues. Solution: Complete rewrite in PyTorch.
            </p>

            <p><b>Limited Strategic Depth</b></p>
            <p>
            The model learns pattern recognition, not strategic planning. It's excellent at tactical moves (blocking immediate threats, taking obvious wins) but struggles with setups requiring 2–3 move foresight. MCTS can think ahead many moves, but the transformer only sees one snapshot.
            </p>

            <p><b>Early-Game Uncertainty</b></p>
            <p>
            In opening positions with many valid moves, the model shows low confidence. Validation accuracy on early-game positions is around 35–40%, while late-game positions achieve 65–75%. This makes sense. Early moves are more “artistic” while late-game moves are more forced.
            </p>
          format: restricted_html
          role: report-body
        type: RichText
      - layout_properties: {grid_position: 'HWKKLF,VBVBFA'}
        name: label_1
        properties: {role: report-subhead, text: Board Analysis}
        type: Label
      - layout_properties: {grid_position: 'ZXYHYO,SNCOXY'}
        name: rich_text_11
        properties:
          content: |
            <p><b>Easy Boards</b></p>
            <p>
            The easiest board position for the transformer to predict is unsurprisingly when there is only one choice to make. In this case the model can’t make the wrong choice because there’s only one move to make.
            </p>
          format: restricted_html
          role: report-body
        type: RichText
      - layout_properties: {grid_position: 'YXTRYU,XYKEGZ'}
        name: image_1
        properties: {display_mode: fill_width, height: 165, source: _/theme/cropped_1.png}
        type: Image
      - layout_properties: {grid_position: 'YXTRYU,MVVAXD'}
        name: image_4
        properties:
          display_mode: fill_width
          height: 363.5
          margin: [0, null, 0, null]
          source: _/theme/cropped_2.png
        type: Image
      - components: []
        layout_properties: {grid_position: 'SDOFYV,YSUWQU'}
        name: rich_text_3
        properties:
          content: |-
            <p><b>Difficult Boards</b></p>
            <p>
            Taking a look at the boards that were the most difficult for the model to predict, we clearly see that it’s when there are many options. If there is no clear win, it’s difficult for the model to predict what the most advantageous move is.
            </p>
          format: restricted_html
          role: report-body
        type: RichText
      - layout_properties: {grid_position: 'SFPGLD,EWVMEP'}
        name: image_5
        properties: {display_mode: fill_width, source: _/theme/cropped_3.png}
        type: Image
      - layout_properties: {grid_position: 'SFPGLD,FKAPBT'}
        name: image_6
        properties: {display_mode: fill_width, source: _/theme/cropped_4.png}
        type: Image
      - layout_properties: {grid_position: 'KJVSTU,HKEXUD'}
        name: label_11
        properties: {role: report-subhead, text: Final Performance}
        type: Label
      - layout_properties: {grid_position: 'RVSBYK,UPPRVX'}
        name: rich_text_12
        properties:
          content: |-
            <p><b>Training Results:</b></p>

            <ul>
              <li>Training Loss: 1.1954</li>
              <li>Training Accuracy: 52.35%</li>
              <li>Validation Loss: 1.2312</li>
              <li>Validation Accuracy: 51.40%</li>
            </ul>

            <div style="height:10px;"></div>

            <p>
            This accuracy is quite good considering the task complexity. With 7 possible moves, random guessing would achieve only 14% accuracy. The model achieves <b>3.6× better than random</b>, demonstrating it has learned meaningful Connect 4 strategy from the MCTS data.
            </p>

            <p>
            The validation accuracy of 51% means the model predicts the exact same move as MCTS more than half the time. Many positions have multiple reasonable moves with similar strategic value, so cases where the model chooses a different (but still good) move are counted as “incorrect” even though they might be perfectly valid plays.
            </p>

            <p>
            The close alignment between training (52.35%) and validation (51.40%) accuracy suggests the model generalizes well without significant overfitting, which validates our choice of dropout (0.2) and model size.
            </p>
          format: restricted_html
          role: report-body
        type: RichText
      layout_properties: {}
      name: column_panel_2
      properties: {role: report-card}
      type: ColumnPanel
    - name: spacer_2
      properties: {height: 32}
      type: Spacer
    layout_properties: {grid_position: 'QSTSNQ,QPROWN'}
    name: Model_Building
    properties: {}
    type: LinearPanel
  layout_properties: {slot: default}
  name: home
  properties: {background: '#1a1a2e', col_widths: '{}', role: page}
  type: ColumnPanel
container:
  properties: {html: '@theme:standard-page.html'}
  type: HtmlTemplate
is_package: true
